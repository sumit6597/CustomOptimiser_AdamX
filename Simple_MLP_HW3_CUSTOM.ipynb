{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_iYcla4kCX67"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\n",
        "\n",
        "\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kgna3kY6CX67",
        "outputId": "64db6c34-f7e0-43a3-bca0-5316b6d83c4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "JodgHy9nCX68",
        "outputId": "64d401bc-a64b-4ab4-cbee-89bbc1822d29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIRI-uLoCX69",
        "outputId": "c7149a12-337c-40b8-f10b-29ecaa03500f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ],
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDyZ8bZjCX69",
        "outputId": "a8fedc52-e018-417f-85ab-a74c37e690ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ],
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lIIy313CX69",
        "outputId": "5bcc5d34-cd1e-4e96-89af-c0edad45329e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD without regularisation"
      ],
      "metadata": {
        "id": "jtrmrW3ZbEK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "pOnhvVlUCX6-"
      },
      "outputs": [],
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(1234)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KAcfDnH7cW3i",
        "outputId": "295e8f31-a821-4f42-9b95-e942ea146f16"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9155\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0037812588500976563 \n",
            "\n",
            "Validation Accuracy: 0.9214\n",
            "\n",
            "Train Accuracy: 0.9365\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0016907225036621094 \n",
            "\n",
            "Validation Accuracy: 0.9419\n",
            "\n",
            "Train Accuracy: 0.9536\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0012759355163574218 \n",
            "\n",
            "Validation Accuracy: 0.9535\n",
            "\n",
            "Train Accuracy: 0.9600\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.001022857437133789 \n",
            "\n",
            "Validation Accuracy: 0.9595\n",
            "\n",
            "Train Accuracy: 0.9659\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0008465966796875 \n",
            "\n",
            "Validation Accuracy: 0.9626\n",
            "\n",
            "Train Accuracy: 0.9708\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0007152566528320312 \n",
            "\n",
            "Validation Accuracy: 0.9650\n",
            "\n",
            "Train Accuracy: 0.9700\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0006099381637573242 \n",
            "\n",
            "Validation Accuracy: 0.9628\n",
            "\n",
            "Train Accuracy: 0.9756\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0005239120864868164 \n",
            "\n",
            "Validation Accuracy: 0.9660\n",
            "\n",
            "Train Accuracy: 0.9820\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.00046186901092529296 \n",
            "\n",
            "Validation Accuracy: 0.9694\n",
            "\n",
            "Train Accuracy: 0.9841\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0003918647003173828 \n",
            "\n",
            "Validation Accuracy: 0.9696\n",
            "\n",
            "Train Accuracy: 0.9827\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.00034398265838623046 \n",
            "\n",
            "Validation Accuracy: 0.9701\n",
            "\n",
            "Train Accuracy: 0.9877\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00029611854553222654 \n",
            "\n",
            "Validation Accuracy: 0.9711\n",
            "\n",
            "Train Accuracy: 0.9888\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00025701711654663086 \n",
            "\n",
            "Validation Accuracy: 0.9717\n",
            "\n",
            "Train Accuracy: 0.9906\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0002207257080078125 \n",
            "\n",
            "Validation Accuracy: 0.9728\n",
            "\n",
            "Train Accuracy: 0.9915\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0001912290382385254 \n",
            "\n",
            "Validation Accuracy: 0.9727\n",
            "\n",
            "Train Accuracy: 0.9928\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016453741073608398 \n",
            "\n",
            "Validation Accuracy: 0.9735\n",
            "\n",
            "Train Accuracy: 0.9926\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.00014079496383666992 \n",
            "\n",
            "Validation Accuracy: 0.9750\n",
            "\n",
            "Train Accuracy: 0.9944\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.00012128957748413086 \n",
            "\n",
            "Validation Accuracy: 0.9745\n",
            "\n",
            "Train Accuracy: 0.9953\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.00010436011314392089 \n",
            "\n",
            "Validation Accuracy: 0.9755\n",
            "\n",
            "Train Accuracy: 0.9966\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 8.684967041015624e-05 \n",
            "\n",
            "Validation Accuracy: 0.9750\n",
            "\n",
            "Total time taken (in seconds): 208.33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "8eNKeELWCX6_",
        "outputId": "f6f85eb7-45fd-48ec-c171-8b7c58fb1391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0243\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSProp without regularisation"
      ],
      "metadata": {
        "id": "yGb12Cb4RbYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ],
      "metadata": {
        "id": "-W1CZOt3Rgc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TfzDHB2sRaHa",
        "outputId": "ffee8b93-f562-4ac3-fdff-eb058ce0b555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9121\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004602061767578125 \n",
            "\n",
            "Validation Accuracy: 0.9235\n",
            "\n",
            "Train Accuracy: 0.9356\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0018716239929199218 \n",
            "\n",
            "Validation Accuracy: 0.9420\n",
            "\n",
            "Train Accuracy: 0.9489\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.001513113250732422 \n",
            "\n",
            "Validation Accuracy: 0.9511\n",
            "\n",
            "Train Accuracy: 0.9561\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0012915208435058593 \n",
            "\n",
            "Validation Accuracy: 0.9567\n",
            "\n",
            "Train Accuracy: 0.9617\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0011517876434326171 \n",
            "\n",
            "Validation Accuracy: 0.9608\n",
            "\n",
            "Train Accuracy: 0.9653\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0010546837615966797 \n",
            "\n",
            "Validation Accuracy: 0.9625\n",
            "\n",
            "Train Accuracy: 0.9688\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0009852069091796875 \n",
            "\n",
            "Validation Accuracy: 0.9641\n",
            "\n",
            "Train Accuracy: 0.9695\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0009350966644287109 \n",
            "\n",
            "Validation Accuracy: 0.9654\n",
            "\n",
            "Train Accuracy: 0.9709\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0009046910095214844 \n",
            "\n",
            "Validation Accuracy: 0.9664\n",
            "\n",
            "Train Accuracy: 0.9723\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0008884773254394531 \n",
            "\n",
            "Validation Accuracy: 0.9658\n",
            "\n",
            "Train Accuracy: 0.9751\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.000886942138671875 \n",
            "\n",
            "Validation Accuracy: 0.9684\n",
            "\n",
            "Train Accuracy: 0.9742\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0008996698760986328 \n",
            "\n",
            "Validation Accuracy: 0.9674\n",
            "\n",
            "Train Accuracy: 0.9768\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.000894249267578125 \n",
            "\n",
            "Validation Accuracy: 0.9687\n",
            "\n",
            "Train Accuracy: 0.9786\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0009213008117675781 \n",
            "\n",
            "Validation Accuracy: 0.9694\n",
            "\n",
            "Train Accuracy: 0.9775\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0009423593902587891 \n",
            "\n",
            "Validation Accuracy: 0.9679\n",
            "\n",
            "Train Accuracy: 0.9790\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0009700937652587891 \n",
            "\n",
            "Validation Accuracy: 0.9680\n",
            "\n",
            "Train Accuracy: 0.9774\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0009666655731201171 \n",
            "\n",
            "Validation Accuracy: 0.9665\n",
            "\n",
            "Train Accuracy: 0.9795\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0009981602478027344 \n",
            "\n",
            "Validation Accuracy: 0.9675\n",
            "\n",
            "Train Accuracy: 0.9770\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0010110160064697267 \n",
            "\n",
            "Validation Accuracy: 0.9668\n",
            "\n",
            "Train Accuracy: 0.9788\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.001033172149658203 \n",
            "\n",
            "Validation Accuracy: 0.9677\n",
            "\n",
            "Total time taken (in seconds): 364.34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JimlKTvYRrzW",
        "outputId": "8882ce2e-cd0f-4bb6-b573-fdc85d4900ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0586\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam without regularisation"
      ],
      "metadata": {
        "id": "vTEVSTsY4ICP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ],
      "metadata": {
        "id": "mAFiUP6wS75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BbrSAnCMTDVL",
        "outputId": "3774be9e-f764-4bd4-b2a7-a792f81eb0e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8690\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.008301019287109374 \n",
            "\n",
            "Validation Accuracy: 0.8858\n",
            "\n",
            "Train Accuracy: 0.9071\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0028919625854492187 \n",
            "\n",
            "Validation Accuracy: 0.9173\n",
            "\n",
            "Train Accuracy: 0.9218\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0022654786682128904 \n",
            "\n",
            "Validation Accuracy: 0.9284\n",
            "\n",
            "Train Accuracy: 0.9298\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0019733512878417968 \n",
            "\n",
            "Validation Accuracy: 0.9357\n",
            "\n",
            "Train Accuracy: 0.9369\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.00178035888671875 \n",
            "\n",
            "Validation Accuracy: 0.9398\n",
            "\n",
            "Train Accuracy: 0.9422\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.001636267547607422 \n",
            "\n",
            "Validation Accuracy: 0.9443\n",
            "\n",
            "Train Accuracy: 0.9457\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0015214817810058595 \n",
            "\n",
            "Validation Accuracy: 0.9459\n",
            "\n",
            "Train Accuracy: 0.9488\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0014254425048828125 \n",
            "\n",
            "Validation Accuracy: 0.9495\n",
            "\n",
            "Train Accuracy: 0.9521\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.001345452423095703 \n",
            "\n",
            "Validation Accuracy: 0.9517\n",
            "\n",
            "Train Accuracy: 0.9552\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0012749099731445312 \n",
            "\n",
            "Validation Accuracy: 0.9538\n",
            "\n",
            "Train Accuracy: 0.9574\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0012110986328125 \n",
            "\n",
            "Validation Accuracy: 0.9561\n",
            "\n",
            "Train Accuracy: 0.9591\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0011615845489501952 \n",
            "\n",
            "Validation Accuracy: 0.9575\n",
            "\n",
            "Train Accuracy: 0.9611\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0011148213958740234 \n",
            "\n",
            "Validation Accuracy: 0.9576\n",
            "\n",
            "Train Accuracy: 0.9627\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0010738916778564452 \n",
            "\n",
            "Validation Accuracy: 0.9592\n",
            "\n",
            "Train Accuracy: 0.9643\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0010335726928710938 \n",
            "\n",
            "Validation Accuracy: 0.9602\n",
            "\n",
            "Train Accuracy: 0.9662\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0009996950531005859 \n",
            "\n",
            "Validation Accuracy: 0.9603\n",
            "\n",
            "Train Accuracy: 0.9676\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0009665296936035156 \n",
            "\n",
            "Validation Accuracy: 0.9619\n",
            "\n",
            "Train Accuracy: 0.9685\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0009406513214111328 \n",
            "\n",
            "Validation Accuracy: 0.9627\n",
            "\n",
            "Train Accuracy: 0.9699\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0009121717071533203 \n",
            "\n",
            "Validation Accuracy: 0.9633\n",
            "\n",
            "Train Accuracy: 0.9706\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0008881653594970704 \n",
            "\n",
            "Validation Accuracy: 0.9637\n",
            "\n",
            "Total time taken (in seconds): 368.97\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOhl9zmfTGvn",
        "outputId": "133ffce8-8870-41b0-8d5a-870d0da2f50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0344\n",
            "\n",
            "Test Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##For FMNIST data"
      ],
      "metadata": {
        "id": "WWoUWl6LUwfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "QeMwqaQyUwQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLfDGSHOUwOd",
        "outputId": "ec934b28-4002-4fed-83cc-911192677395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUeCrwovVQ-V",
        "outputId": "7706d387-2924-4c8c-d1e4-e118d5058339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ObYZWM2VQ79",
        "outputId": "fa417355-c855-48c7-e4cb-92df23809d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SGD without regularisation"
      ],
      "metadata": {
        "id": "CYM0rtV1bi9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ],
      "metadata": {
        "id": "aeRX57I2VZ6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PJ2TOAqgVZ39",
        "outputId": "ff8fb814-9629-49ef-ae3a-c0941589c33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8349\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.005419139404296875 \n",
            "\n",
            "Validation Accuracy: 0.8317\n",
            "\n",
            "Train Accuracy: 0.8460\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0036072598266601564 \n",
            "\n",
            "Validation Accuracy: 0.8421\n",
            "\n",
            "Train Accuracy: 0.8662\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0031853564453125 \n",
            "\n",
            "Validation Accuracy: 0.8568\n",
            "\n",
            "Train Accuracy: 0.8705\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0029390103149414064 \n",
            "\n",
            "Validation Accuracy: 0.8589\n",
            "\n",
            "Train Accuracy: 0.8774\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0027496945190429688 \n",
            "\n",
            "Validation Accuracy: 0.8658\n",
            "\n",
            "Train Accuracy: 0.8833\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0026207452392578126 \n",
            "\n",
            "Validation Accuracy: 0.8698\n",
            "\n",
            "Train Accuracy: 0.8878\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002501474609375 \n",
            "\n",
            "Validation Accuracy: 0.8717\n",
            "\n",
            "Train Accuracy: 0.8917\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0024232452392578124 \n",
            "\n",
            "Validation Accuracy: 0.8732\n",
            "\n",
            "Train Accuracy: 0.8858\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002325525970458984 \n",
            "\n",
            "Validation Accuracy: 0.8690\n",
            "\n",
            "Train Accuracy: 0.8988\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0022495370483398437 \n",
            "\n",
            "Validation Accuracy: 0.8776\n",
            "\n",
            "Train Accuracy: 0.8994\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.002175191192626953 \n",
            "\n",
            "Validation Accuracy: 0.8758\n",
            "\n",
            "Train Accuracy: 0.8998\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.002095566864013672 \n",
            "\n",
            "Validation Accuracy: 0.8767\n",
            "\n",
            "Train Accuracy: 0.9024\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0020491122436523436 \n",
            "\n",
            "Validation Accuracy: 0.8762\n",
            "\n",
            "Train Accuracy: 0.9048\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.001981264953613281 \n",
            "\n",
            "Validation Accuracy: 0.8803\n",
            "\n",
            "Train Accuracy: 0.9089\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0019318571472167968 \n",
            "\n",
            "Validation Accuracy: 0.8805\n",
            "\n",
            "Train Accuracy: 0.9111\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0018846038818359375 \n",
            "\n",
            "Validation Accuracy: 0.8794\n",
            "\n",
            "Train Accuracy: 0.9129\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0018338267517089843 \n",
            "\n",
            "Validation Accuracy: 0.8825\n",
            "\n",
            "Train Accuracy: 0.9112\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0017673365783691406 \n",
            "\n",
            "Validation Accuracy: 0.8804\n",
            "\n",
            "Train Accuracy: 0.9188\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0017132736206054687 \n",
            "\n",
            "Validation Accuracy: 0.8833\n",
            "\n",
            "Train Accuracy: 0.9132\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0016818855285644532 \n",
            "\n",
            "Validation Accuracy: 0.8774\n",
            "\n",
            "Total time taken (in seconds): 220.41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "id": "wCBkMbb1VnIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSProp without regularisation"
      ],
      "metadata": {
        "id": "m8KG2tuMWiis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ],
      "metadata": {
        "id": "eHkLnTY7VnGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "id": "z_E2XQ--WoCv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e4401de-a833-4deb-e39e-613721a0748e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8265\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0053944573974609375 \n",
            "\n",
            "Validation Accuracy: 0.8212\n",
            "\n",
            "Train Accuracy: 0.8354\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0035924508666992185 \n",
            "\n",
            "Validation Accuracy: 0.8277\n",
            "\n",
            "Train Accuracy: 0.8565\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0032489251708984375 \n",
            "\n",
            "Validation Accuracy: 0.8505\n",
            "\n",
            "Train Accuracy: 0.8662\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0030573263549804686 \n",
            "\n",
            "Validation Accuracy: 0.8628\n",
            "\n",
            "Train Accuracy: 0.8743\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0029266281127929687 \n",
            "\n",
            "Validation Accuracy: 0.8626\n",
            "\n",
            "Train Accuracy: 0.8772\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0028465325927734377 \n",
            "\n",
            "Validation Accuracy: 0.8659\n",
            "\n",
            "Train Accuracy: 0.8787\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.002782617492675781 \n",
            "\n",
            "Validation Accuracy: 0.8662\n",
            "\n",
            "Train Accuracy: 0.8809\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002767595520019531 \n",
            "\n",
            "Validation Accuracy: 0.8667\n",
            "\n",
            "Train Accuracy: 0.8823\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002756751708984375 \n",
            "\n",
            "Validation Accuracy: 0.8670\n",
            "\n",
            "Train Accuracy: 0.8833\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0027674874877929687 \n",
            "\n",
            "Validation Accuracy: 0.8693\n",
            "\n",
            "Train Accuracy: 0.8842\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0028095257568359375 \n",
            "\n",
            "Validation Accuracy: 0.8674\n",
            "\n",
            "Train Accuracy: 0.8836\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0028801348876953126 \n",
            "\n",
            "Validation Accuracy: 0.8674\n",
            "\n",
            "Train Accuracy: 0.8849\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00296575927734375 \n",
            "\n",
            "Validation Accuracy: 0.8651\n",
            "\n",
            "Train Accuracy: 0.8849\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0030671710205078125 \n",
            "\n",
            "Validation Accuracy: 0.8700\n",
            "\n",
            "Train Accuracy: 0.8849\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0031474542236328124 \n",
            "\n",
            "Validation Accuracy: 0.8671\n",
            "\n",
            "Train Accuracy: 0.8823\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0032426373291015624 \n",
            "\n",
            "Validation Accuracy: 0.8628\n",
            "\n",
            "Train Accuracy: 0.8783\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0033364697265625 \n",
            "\n",
            "Validation Accuracy: 0.8633\n",
            "\n",
            "Train Accuracy: 0.8751\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0034896029663085935 \n",
            "\n",
            "Validation Accuracy: 0.8573\n",
            "\n",
            "Train Accuracy: 0.8789\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0036107998657226564 \n",
            "\n",
            "Validation Accuracy: 0.8618\n",
            "\n",
            "Train Accuracy: 0.8637\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.003778652648925781 \n",
            "\n",
            "Validation Accuracy: 0.8443\n",
            "\n",
            "Total time taken (in seconds): 357.04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "id": "p9igW4dSWunO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9f1ca87-0b7f-4b56-94e2-fa6d3e9b001b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1823\n",
            "\n",
            "Test Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam without regularisation"
      ],
      "metadata": {
        "id": "91vhDEh8bqO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # optimizer = AdamX('AdamX')\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train) #forward pass\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)   #grad type is list of len(8)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance \n",
        "\n"
      ],
      "metadata": {
        "id": "SdJLD25NXJfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5052)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "  \n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "id": "isRf5NdyXJdK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2b81c0c-1331-4182-c021-28225143ac00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.7969\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0078467138671875 \n",
            "\n",
            "Validation Accuracy: 0.7899\n",
            "\n",
            "Train Accuracy: 0.8253\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.004235588989257812 \n",
            "\n",
            "Validation Accuracy: 0.8216\n",
            "\n",
            "Train Accuracy: 0.8409\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0037437509155273437 \n",
            "\n",
            "Validation Accuracy: 0.8345\n",
            "\n",
            "Train Accuracy: 0.8469\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.00349382568359375 \n",
            "\n",
            "Validation Accuracy: 0.8392\n",
            "\n",
            "Train Accuracy: 0.8533\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0033250341796875 \n",
            "\n",
            "Validation Accuracy: 0.8455\n",
            "\n",
            "Train Accuracy: 0.8589\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0032033493041992186 \n",
            "\n",
            "Validation Accuracy: 0.8517\n",
            "\n",
            "Train Accuracy: 0.8631\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0031061029052734373 \n",
            "\n",
            "Validation Accuracy: 0.8545\n",
            "\n",
            "Train Accuracy: 0.8659\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0030265219116210936 \n",
            "\n",
            "Validation Accuracy: 0.8581\n",
            "\n",
            "Train Accuracy: 0.8682\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.0029523782348632815 \n",
            "\n",
            "Validation Accuracy: 0.8606\n",
            "\n",
            "Train Accuracy: 0.8722\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0028922137451171874 \n",
            "\n",
            "Validation Accuracy: 0.8632\n",
            "\n",
            "Train Accuracy: 0.8738\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.002842579345703125 \n",
            "\n",
            "Validation Accuracy: 0.8616\n",
            "\n",
            "Train Accuracy: 0.8760\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.0027924966430664063 \n",
            "\n",
            "Validation Accuracy: 0.8633\n",
            "\n",
            "Train Accuracy: 0.8779\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.0027546380615234376 \n",
            "\n",
            "Validation Accuracy: 0.8645\n",
            "\n",
            "Train Accuracy: 0.8791\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0027127496337890624 \n",
            "\n",
            "Validation Accuracy: 0.8659\n",
            "\n",
            "Train Accuracy: 0.8812\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.002684827880859375 \n",
            "\n",
            "Validation Accuracy: 0.8667\n",
            "\n",
            "Train Accuracy: 0.8822\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.0026562057495117188 \n",
            "\n",
            "Validation Accuracy: 0.8669\n",
            "\n",
            "Train Accuracy: 0.8837\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0026274557495117186 \n",
            "\n",
            "Validation Accuracy: 0.8692\n",
            "\n",
            "Train Accuracy: 0.8855\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.002604492492675781 \n",
            "\n",
            "Validation Accuracy: 0.8694\n",
            "\n",
            "Train Accuracy: 0.8868\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.002576748962402344 \n",
            "\n",
            "Validation Accuracy: 0.8711\n",
            "\n",
            "Train Accuracy: 0.8875\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0025538790893554686 \n",
            "\n",
            "Validation Accuracy: 0.8725\n",
            "\n",
            "Total time taken (in seconds): 361.26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzftTLT-XJa8",
        "outputId": "917bd5fb-1ff0-49b4-9dfb-f2f3fee2da46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0988\n",
            "\n",
            "Test Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Optimiser on Fashion MNIST"
      ],
      "metadata": {
        "id": "lOCFCtn0ChGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "XQz1pCEgVANy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "ecbd9f4f-7e9f-41bf-977e-d6890a3f02b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ],
      "metadata": {
        "id": "nVy1t5nlVALt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "517b73b7-0afc-40aa-9954-f63abe9a791b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xYsXWqI_CX6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72faa2eb-908b-4e96-fda6-111ee71d4782"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaB6K_vlBV_c",
        "outputId": "4065d549-cdf9-418c-e41c-3cf654cdad5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    # with tf.GradientTape() as tape:\n",
        "        \n",
        "    #   predicted = self.forward(X_train)\n",
        "    #   current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    # grads = tape.gradient(current_loss, self.variables)\n",
        "    # optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "      \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    return grads, self.variables\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n"
      ],
      "metadata": {
        "id": "NvrRhT7kCwij"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamX(OptimizerV2):\n",
        "\n",
        "    def __init__(self,lr=1e-2,beta1=0.9,beta2=0.999,beta3=0.999987,eps=1e-8,wt_decay=0.0,name='AdamX', **kwargs):\n",
        "        super(AdamX, self).__init__(name, **kwargs)\n",
        "\n",
        "        self._set_hyper('lr', kwargs.get('lr', lr))\n",
        "        self._set_hyper('beta1', beta1)\n",
        "        self._set_hyper('beta2', beta2)\n",
        "        self._set_hyper('beta3', beta3)\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self.eps = eps\n",
        "        self.wt_decay = wt_decay\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, gvs):#_resource_apply_dense (update variable given gradient tensor is a dense tf.Tensor)\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'm') #A slot variable is an additional variable associated with var to train. It is allocated and managed by optimizers, e.g. Adam\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'v')\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'u')\n",
        "        var_dtype = var[0].dtype.base_dtype\n",
        "        \n",
        "        b1t = self._get_hyper('beta1', var_dtype) # If they are callable, the callable will be called during apply_gradients() to get the value for the hyper parameter.\n",
        "        b2t = self._get_hyper('beta2', var_dtype)\n",
        "        b3t = self._get_hyper('beta3', var_dtype)\n",
        "        lr = self._get_hyper('lr', var_dtype)\n",
        "\n",
        "        iter = tf.cast(self.iterations + 1, var_dtype) #iteration is a variable which keeps number of training steps the Optimizer has run.\n",
        "        operation = []\n",
        "\n",
        "        for (g,var) in gvs:\n",
        "          m = self.get_slot(var, 'm') # the moment vectors using get_slot\n",
        "          v = self.get_slot(var, 'v')\n",
        "          u = self.get_slot(var, 'u')\n",
        "\n",
        "          m_t = (b1t * m) + (1. - b1t) * g\n",
        "          v_t = (b2t * v) + (1. - b2t) * tf.square(g)\n",
        "          u_t = (b3t * u) + (1. - b3t)* g**3\n",
        "          b1t = b1t ** iter\n",
        "          b2t = b2t ** iter\n",
        "          b3t = b3t ** iter\n",
        "          \n",
        "          m_t_h = m_t/(1-b1t)\n",
        "          v_t_h = v_t/(1-b2t)\n",
        "          u_t_h = u_t/(1-b3t)\n",
        "\n",
        "          m_t = tf.compat.v1.assign(m, m_t)\n",
        "          v_t = tf.compat.v1.assign(v, v_t)\n",
        "          u_t = tf.compat.v1.assign(u, u_t)\n",
        "\n",
        "\n",
        "          update1 = -lr*m_t_h\n",
        "          update=update1/(tf.sqrt(v_t_h) + np.cbrt(u_t_h)*self.eps + 1e-5)\n",
        "          \n",
        "          operation.append(var.assign_add(update))\n",
        "          output=tf.group(*operation)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "Q3HoywvmCynS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AdamX(lr=1e-2,beta1=0.9,beta2=0.999,beta3=0.999987,eps=1e-8,wt_decay=0.0,name='AdamX')"
      ],
      "metadata": {
        "id": "g8uM6VRqRDJI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "\n",
        "time_start = time.time()\n",
        "acc = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5997)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "    g,v = mlp_on_cpu.backward(inputs, outputs)\n",
        "    gvs=zip(g, v)\n",
        "    model._resource_apply_dense(g,v,gvs)\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  acc.append(ds)\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RgB1bl9MC0i8",
        "outputId": "ce6e93cd-b3d6-4120-cf53-2b74d8142c0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8511\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.0043425485229492185 \n",
            "\n",
            "Validation Accuracy: 0.8428\n",
            "\n",
            "Train Accuracy: 0.8702\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0030211819458007814 \n",
            "\n",
            "Validation Accuracy: 0.8573\n",
            "\n",
            "Train Accuracy: 0.8726\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0027644732666015624 \n",
            "\n",
            "Validation Accuracy: 0.8543\n",
            "\n",
            "Train Accuracy: 0.8802\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002605854797363281 \n",
            "\n",
            "Validation Accuracy: 0.8597\n",
            "\n",
            "Train Accuracy: 0.8836\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.0024745721435546876 \n",
            "\n",
            "Validation Accuracy: 0.8631\n",
            "\n",
            "Train Accuracy: 0.8927\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.0023851483154296877 \n",
            "\n",
            "Validation Accuracy: 0.8703\n",
            "\n",
            "Train Accuracy: 0.8952\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0022961929321289065 \n",
            "\n",
            "Validation Accuracy: 0.8728\n",
            "\n",
            "Train Accuracy: 0.8950\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.002217744293212891 \n",
            "\n",
            "Validation Accuracy: 0.8731\n",
            "\n",
            "Train Accuracy: 0.8961\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.002149217681884766 \n",
            "\n",
            "Validation Accuracy: 0.8733\n",
            "\n",
            "Train Accuracy: 0.9015\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0020873117065429688 \n",
            "\n",
            "Validation Accuracy: 0.8729\n",
            "\n",
            "Train Accuracy: 0.8992\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0020596170043945313 \n",
            "\n",
            "Validation Accuracy: 0.8720\n",
            "\n",
            "Train Accuracy: 0.9039\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.001989237823486328 \n",
            "\n",
            "Validation Accuracy: 0.8782\n",
            "\n",
            "Train Accuracy: 0.9044\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.001954282684326172 \n",
            "\n",
            "Validation Accuracy: 0.8745\n",
            "\n",
            "Train Accuracy: 0.9076\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0019199325561523439 \n",
            "\n",
            "Validation Accuracy: 0.8772\n",
            "\n",
            "Train Accuracy: 0.9067\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.0019256846618652343 \n",
            "\n",
            "Validation Accuracy: 0.8742\n",
            "\n",
            "Train Accuracy: 0.9092\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00187911376953125 \n",
            "\n",
            "Validation Accuracy: 0.8764\n",
            "\n",
            "Train Accuracy: 0.9090\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0018640567016601562 \n",
            "\n",
            "Validation Accuracy: 0.8762\n",
            "\n",
            "Train Accuracy: 0.9111\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0018234706115722657 \n",
            "\n",
            "Validation Accuracy: 0.8751\n",
            "\n",
            "Train Accuracy: 0.9149\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0017815692138671874 \n",
            "\n",
            "Validation Accuracy: 0.8786\n",
            "\n",
            "Train Accuracy: 0.9118\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.0018057740783691406 \n",
            "\n",
            "Validation Accuracy: 0.8774\n",
            "\n",
            "Total time taken (in seconds): 644.85\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzpmge-AG0qV",
        "outputId": "32676cb9-7bd0-4199-fb08-6475e2c1ec65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.1039\n",
            "\n",
            "Test Accuracy: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Custom Optimiser on Digit MNIST"
      ],
      "metadata": {
        "id": "pOoFwcnMDbhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "98wJbx5vC2me",
        "outputId": "819068d2-2307-4df2-fa7b-3efffb25f77f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVvZtEBmDpgA",
        "outputId": "f87ab785-3ca3-4cf2-a826-6426a6206eb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JrTTWm5D346",
        "outputId": "413b7765-02bd-4ef9-82c4-e36b47b7b3d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp2C9HuCD4gk",
        "outputId": "a51a1b31-22c7-42ed-b0ff-cb85b1d9b8fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "    \n",
        "    # with tf.GradientTape() as tape:\n",
        "        \n",
        "    #   predicted = self.forward(X_train)\n",
        "    #   current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    # grads = tape.gradient(current_loss, self.variables)\n",
        "    # optimizer.apply_gradients(zip(grads, self.variables))\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    # optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      # print('OK')\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "      # print('OK')\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    # print('OK')\n",
        "    return grads, self.variables\n",
        "    # print(grads)\n",
        "\n",
        "    # AdamOptimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n"
      ],
      "metadata": {
        "id": "Q-6NDhp5GTlT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdamX(OptimizerV2):\n",
        "\n",
        "    def __init__(self,lr=1e-2,beta1=0.9,beta2=0.999,beta3=0.999987,eps=1e-8,wt_decay=0.0,name='AdamX', **kwargs):\n",
        "        super(AdamX, self).__init__(name, **kwargs)\n",
        "\n",
        "        self._set_hyper('lr', kwargs.get('lr', lr))\n",
        "        self._set_hyper('beta1', beta1)\n",
        "        self._set_hyper('beta2', beta2)\n",
        "        self._set_hyper('beta3', beta3)\n",
        "        self._set_hyper('decay', self._initial_decay)\n",
        "        self.eps = eps\n",
        "        self.wt_decay = wt_decay\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, gvs):#_resource_apply_dense (update variable given gradient tensor is a dense tf.Tensor)\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'm') #A slot variable is an additional variable associated with var to train. It is allocated and managed by optimizers, e.g. Adam\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'v')\n",
        "        for item in var:\n",
        "            self.add_slot(item, 'u')\n",
        "        var_dtype = var[0].dtype.base_dtype\n",
        "        \n",
        "        b1t = self._get_hyper('beta1', var_dtype) # If they are callable, the callable will be called during apply_gradients() to get the value for the hyper parameter.\n",
        "        b2t = self._get_hyper('beta2', var_dtype)\n",
        "        b3t = self._get_hyper('beta3', var_dtype)\n",
        "        lr = self._get_hyper('lr', var_dtype)\n",
        "\n",
        "        iter = tf.cast(self.iterations + 1, var_dtype) #iteration is a variable which keeps number of training steps the Optimizer has run.\n",
        "        operation = []\n",
        "\n",
        "        for (g,var) in gvs:\n",
        "          m = self.get_slot(var, 'm') # the moment vectors using get_slot\n",
        "          v = self.get_slot(var, 'v')\n",
        "          u = self.get_slot(var, 'u')\n",
        "\n",
        "          m_t = (b1t * m) + (1. - b1t) * g\n",
        "          v_t = (b2t * v) + (1. - b2t) * tf.square(g)\n",
        "          u_t = (b3t * u) + (1. - b3t)* g**3\n",
        "          b1t = b1t ** iter\n",
        "          b2t = b2t ** iter\n",
        "          b3t = b3t ** iter\n",
        "          \n",
        "          m_t_h = m_t/(1-b1t)\n",
        "          v_t_h = v_t/(1-b2t)\n",
        "          u_t_h = u_t/(1-b3t)\n",
        "\n",
        "          m_t = tf.compat.v1.assign(m, m_t)\n",
        "          v_t = tf.compat.v1.assign(v, v_t)\n",
        "          u_t = tf.compat.v1.assign(u, u_t)\n",
        "\n",
        "\n",
        "          update1 = -lr*m_t_h\n",
        "          update=update1/(tf.sqrt(v_t_h) + np.cbrt(u_t_h)*self.eps + 1e-5)\n",
        "          \n",
        "          operation.append(var.assign_add(update))\n",
        "          output=tf.group(*operation)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "AtW9FzfoGY-t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AdamX(lr=1e-2,beta1=0.9,beta2=0.999,beta3=0.999987,eps=1e-8,wt_decay=0.0,name='AdamX')"
      ],
      "metadata": {
        "id": "WlttF0KA38Tu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "\n",
        "\n",
        "time_start = time.time()\n",
        "acc = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(5997)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs) \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, outputs)\n",
        "\n",
        "    g,v = mlp_on_cpu.backward(inputs, outputs)\n",
        "    model._resource_apply_dense(g,v,zip(g, v))\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train)\n",
        "\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  acc.append(ds)\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "  preds_val = mlp_on_cpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kJqseT1VGc10",
        "outputId": "ad98a247-5b3e-4de8-ff29-28ed332ee96e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.9603\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.002344550018310547 \n",
            "\n",
            "Validation Accuracy: 0.9588\n",
            "\n",
            "Train Accuracy: 0.9749\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0009940172576904298 \n",
            "\n",
            "Validation Accuracy: 0.9681\n",
            "\n",
            "Train Accuracy: 0.9800\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0006922158813476562 \n",
            "\n",
            "Validation Accuracy: 0.9716\n",
            "\n",
            "Train Accuracy: 0.9817\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.0005404388046264648 \n",
            "\n",
            "Validation Accuracy: 0.9696\n",
            "\n",
            "Train Accuracy: 0.9841\n",
            "Number of Epoch = 5 - Average Cross Entropy:= 0.00043491741180419924 \n",
            "\n",
            "Validation Accuracy: 0.9702\n",
            "\n",
            "Train Accuracy: 0.9866\n",
            "Number of Epoch = 6 - Average Cross Entropy:= 0.00036355602264404296 \n",
            "\n",
            "Validation Accuracy: 0.9706\n",
            "\n",
            "Train Accuracy: 0.9875\n",
            "Number of Epoch = 7 - Average Cross Entropy:= 0.0003347264099121094 \n",
            "\n",
            "Validation Accuracy: 0.9729\n",
            "\n",
            "Train Accuracy: 0.9864\n",
            "Number of Epoch = 8 - Average Cross Entropy:= 0.0002880393028259277 \n",
            "\n",
            "Validation Accuracy: 0.9705\n",
            "\n",
            "Train Accuracy: 0.9893\n",
            "Number of Epoch = 9 - Average Cross Entropy:= 0.00025640338897705076 \n",
            "\n",
            "Validation Accuracy: 0.9708\n",
            "\n",
            "Train Accuracy: 0.9898\n",
            "Number of Epoch = 10 - Average Cross Entropy:= 0.0002522258186340332 \n",
            "\n",
            "Validation Accuracy: 0.9712\n",
            "\n",
            "Train Accuracy: 0.9885\n",
            "Number of Epoch = 11 - Average Cross Entropy:= 0.0002121415328979492 \n",
            "\n",
            "Validation Accuracy: 0.9713\n",
            "\n",
            "Train Accuracy: 0.9896\n",
            "Number of Epoch = 12 - Average Cross Entropy:= 0.00019793960571289063 \n",
            "\n",
            "Validation Accuracy: 0.9696\n",
            "\n",
            "Train Accuracy: 0.9929\n",
            "Number of Epoch = 13 - Average Cross Entropy:= 0.00017328741073608398 \n",
            "\n",
            "Validation Accuracy: 0.9736\n",
            "\n",
            "Train Accuracy: 0.9901\n",
            "Number of Epoch = 14 - Average Cross Entropy:= 0.0001998428153991699 \n",
            "\n",
            "Validation Accuracy: 0.9743\n",
            "\n",
            "Train Accuracy: 0.9915\n",
            "Number of Epoch = 15 - Average Cross Entropy:= 0.00017882349014282226 \n",
            "\n",
            "Validation Accuracy: 0.9699\n",
            "\n",
            "Train Accuracy: 0.9947\n",
            "Number of Epoch = 16 - Average Cross Entropy:= 0.00016075206756591796 \n",
            "\n",
            "Validation Accuracy: 0.9731\n",
            "\n",
            "Train Accuracy: 0.9895\n",
            "Number of Epoch = 17 - Average Cross Entropy:= 0.0001639297866821289 \n",
            "\n",
            "Validation Accuracy: 0.9700\n",
            "\n",
            "Train Accuracy: 0.9940\n",
            "Number of Epoch = 18 - Average Cross Entropy:= 0.0001565926742553711 \n",
            "\n",
            "Validation Accuracy: 0.9715\n",
            "\n",
            "Train Accuracy: 0.9934\n",
            "Number of Epoch = 19 - Average Cross Entropy:= 0.0001262981414794922 \n",
            "\n",
            "Validation Accuracy: 0.9708\n",
            "\n",
            "Train Accuracy: 0.9933\n",
            "Number of Epoch = 20 - Average Cross Entropy:= 0.00014493435859680177 \n",
            "\n",
            "Validation Accuracy: 0.9729\n",
            "\n",
            "Total time taken (in seconds): 525.83\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATcklEQVR4nO3de4xc5XnH8e/jC0hOIoeLRV3AXjdYkYysErRCaZVGkZwGg0pMKhQZrVq3QdpGAimoV1NLSUBaKaRKQK0g1aZQKFrFUJI0piIl1ETKPw2wRg7GEJcN2AbLgAPIpLIEGJ7+Mccw7zC7nvXMztnL9yON9sx73nPmOcfH85tzm4nMRJKkExbVXYAkaXYxGCRJBYNBklQwGCRJBYNBklRYUncBvXD22WfnwMBA3WVI0pyya9euX2fmitb2eREMAwMDjI+P112GJM0pEXGgXbuHkiRJBYNBklQwGCRJBYNBklQwGCRJhQUbDGN7xhi4dYBFNy5i4NYBxvaM1V2SJM0K8+Jy1eka2zPG8APDHHv7GAAHjh5g+IFhAIbWD9VZmiTVbkHuMWzbue29UDjh2NvH2LZzW00VSdLssSCD4eDRg9Nql6SFZEEGw6rlq6bVLkkLyYIMhpENIyxbuqxoW7Z0GSMbRmqqSJJmjwUZDEPrhxi9YpTVy1cTBKuXr2b0ilFPPEsSEPPhN58HBwfTL9GTpOmJiF2ZOdjaviD3GCRJkzMYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVDAYJEkFg0GSVOgoGCJiY0Tsi4iJiNjaZvzpEXFvNf7RiBhoGndD1b4vIi6t2s6PiJ9GxNMRsTcivtLU/8yIeDginq3+ntH9YkqSOnXSYIiIxcBtwGXAOuDqiFjX0u0a4PXMvAC4Bbi5mnYdsBm4ENgI3F7N7zjwV5m5DvgkcG3TPLcCOzNzLbCzei5J6pNO9hguASYy87nMfAvYDmxq6bMJuLsavh/YEBFRtW/PzDcz83lgArgkMw9n5hMAmfkb4Bng3Dbzuhu48tQWTZJ0KjoJhnOBF5qev8j7b+If6JOZx4GjwFmdTFsddvoE8GjVdE5mHq6GXwLOaVdURAxHxHhEjB85cqSDxZAkdaLWk88R8WHg+8D1mflG6/jMTCDbTZuZo5k5mJmDK1asmOFKJWnh6CQYDgHnNz0/r2pr2ycilgDLgVenmjYiltIIhbHM/EFTn5cjYmXVZyXwSqcLI0nqXifB8DiwNiLWRMRpNE4m72jpswPYUg1fBTxSfdrfAWyurlpaA6wFHqvOP9wBPJOZ355iXluAH013oSRJp27JyTpk5vGIuA54CFgM3JmZeyPiJmA8M3fQeJO/JyImgNdohAdVv/uAp2lciXRtZr4TEZ8C/gTYExG7q5f6+8x8EPgGcF9EXAMcAL7YywWWJE0tGh/s57bBwcEcHx+vuwxJmlMiYldmDra2e+ezJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKlgMEiSCgaDJKnQUTBExMaI2BcRExGxtc340yPi3mr8oxEx0DTuhqp9X0Rc2tR+Z0S8EhFPtczr6xFxKCJ2V4/LT33xJEnTddJgiIjFwG3AZcA64OqIWNfS7Rrg9cy8ALgFuLmadh2wGbgQ2AjcXs0P4K6qrZ1bMvOi6vHg9BZJktSNTvYYLgEmMvO5zHwL2A5saumzCbi7Gr4f2BARUbVvz8w3M/N5YKKaH5n5M+C1HiyDJKmHOgmGc4EXmp6/WLW17ZOZx4GjwFkdTtvOdRHxZHW46YwO+kuSemQ2nnz+DvAx4CLgMPCtdp0iYjgixiNi/MiRI/2sT5LmtU6C4RBwftPz86q2tn0iYgmwHHi1w2kLmflyZr6Tme8C36U69NSm32hmDmbm4IoVKzpYDElSJzoJhseBtRGxJiJOo3EyeUdLnx3Almr4KuCRzMyqfXN11dIaYC3w2FQvFhErm55+AXhqsr6SpN5bcrIOmXk8Iq4DHgIWA3dm5t6IuAkYz8wdwB3APRExQeOE8uZq2r0RcR/wNHAcuDYz3wGIiO8BnwHOjogXga9l5h3ANyPiIiCB/cBf9HKBJUlTi8YH+7ltcHAwx8fH6y5DkuaUiNiVmYOt7bPx5LMkqUYGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySpYDBIkgoGgySp0FEwRMTGiNgXERMRsbXN+NMj4t5q/KMRMdA07oaqfV9EXNrUfmdEvBIRT7XM68yIeDginq3+nnHqiydJmq6TBkNELAZuAy4D1gFXR8S6lm7XAK9n5gXALcDN1bTrgM3AhcBG4PZqfgB3VW2ttgI7M3MtsLN6PuuM7Rlj4NYBFt24iIFbBxjbM1Z3SZLUE53sMVwCTGTmc5n5FrAd2NTSZxNwdzV8P7AhIqJq356Zb2bm88BENT8y82fAa21er3ledwNXTmN5+mJszxjDDwxz4OgBkuTA0QMMPzBsOEiaFzoJhnOBF5qev1i1te2TmceBo8BZHU7b6pzMPFwNvwSc065TRAxHxHhEjB85cqSDxeidbTu3ceztY0XbsbePsW3ntr7WIUkzYVaffM7MBHKScaOZOZiZgytWrOhrXQePHpxWuyTNJZ0EwyHg/Kbn51VtbftExBJgOfBqh9O2ejkiVlbzWgm80kGNfbVq+apptUvSXNJJMDwOrI2INRFxGo2TyTta+uwAtlTDVwGPVJ/2dwCbq6uW1gBrgcdO8nrN89oC/KiDGvtqZMMIy5YuK9qWLV3GyIaRmiqSpN45aTBU5wyuAx4CngHuy8y9EXFTRHy+6nYHcFZETAB/SXUlUWbuBe4Dngb+C7g2M98BiIjvAf8DfDwiXoyIa6p5fQP4w4h4Fvhs9XxWGVo/xOgVo6xevpogWL18NaNXjDK0fqju0iSpa9H4YD+3DQ4O5vj4eN1lSNKcEhG7MnOwtX1Wn3yWJPWfwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMkqSCwSBJKhgMNRnbM8bArQMsunERA7cOMLZnrO6SJAmAJXUXsBCN7Rlj+IFhjr19DIADRw8w/MAwAEPrh+osTZLcY6jDtp3b3guFE469fYxtO7fVVJEkvc9gqMHBowen1S5J/WQw1GDV8lXTapekfjIYajCyYYRlS5cVbcuWLmNkw0hNFUnS+wyGGgytH2L0ilFWL19NEKxevprRK0Y98SxpVojMrLuGrg0ODub4+HjdZUjSnBIRuzJzsLXdPQZJUsFgkCQVDAZJUsFgkCQVDAZJUsFgkCQVDIY5ym9nlTRT/HbVOchvZ5U0k9xjmIP8dlZJM6mjYIiIjRGxLyImImJrm/GnR8S91fhHI2KgadwNVfu+iLj0ZPOMiLsi4vmI2F09LupuEecfv51V0kw6aTBExGLgNuAyYB1wdUSsa+l2DfB6Zl4A3ALcXE27DtgMXAhsBG6PiMUdzPNvMvOi6rG7qyWch/x2VkkzqZM9hkuAicx8LjPfArYDm1r6bALurobvBzZERFTt2zPzzcx8Hpio5tfJPDUJv51V0kzqJBjOBV5oev5i1da2T2YeB44CZ00x7cnmORIRT0bELRFxegc1Lih+O6ukmTQbr0q6AXgJOA0YBf4OuKm1U0QMA8MAq1YtvEMoQ+uHDAJJM6KTPYZDwPlNz8+r2tr2iYglwHLg1SmmnXSemXk4G94E/pXGYacPyMzRzBzMzMEVK1Z0sBiSpE50EgyPA2sjYk1EnEbjZPKOlj47gC3V8FXAI9n4oYcdwObqqqU1wFrgsanmGRErq78BXAk81c0Cqj1vkJM0mZMeSsrM4xFxHfAQsBi4MzP3RsRNwHhm7gDuAO6JiAngNRpv9FT97gOeBo4D12bmOwDt5lm95FhErAAC2A18uXeLK/AGOUlT8xfcFqCBWwc4cPTAB9pXL1/N/uv3978gSbXwF9z0Hm+QkzQVg2EB8gY5SVMxGBYgb5CTNBWDYQHyBjlJU/Hks07J2J4xtu3cxsGjB1m1fBUjG0YMFmmOmezk82y881mznJe7SvObh5I0bb34PQhvsJNmL/cYNG3dXu7qHoc0u7nHoGnr9nJXf4FOmt0MBk1bt5e7eoOdNLsZDJq2bi939QY7aXYzGHRKhtYPsf/6/bz7tXfZf/3+aZ0b6MUNdp68lmaOwaC+63aP48TJ6wNHD5DkeyevDQepN7zBTXNOL74d1hv0JL9dVfNIry6X7WaPw0NZms8MBs05dV8u66EszXcGg+acui+X9T4MzXcGg+acui+X7cV9GB6K0mxmMGhOqvNy2W6DZT4cijLY5jeDQQtOt3sc3QbLXP8SwvkQbJqal6tKp6Cby10X3biI5IP/74Lg3a+929FrN38JITSCqV8/tuTlwvPHZJerGgxSn3X7xlr3G/NcDza9z/sYpFmi7ququj0UVPflwtD9obS6p5/tDAapz+q+qqrbN+a5Hmx1T98rMxlOHkqS5phuD8V0eyjoRA2neiiq7kNpdU/fC706HOehJGmeqHuP40QNdV0u3O0eR93Tn9DNJ/6ZvsnSYJDmoLq/9rwbdQdb3dND94ejZvrHrgwGaYHp9o25VzXUFWx1Tw/df+Kf6R+7WtKTuUiaU4bWD83ZS0NP1H2q5zjqnh66/8Q/smGk7TmGXu31efJZkvqs7ntRTpjs5LN7DJLUZ734xD+Te32eY5CkPpsN53mm4qEkSVqgvI9BktQRg0GSVDAYJEkFg0GSVDAYJEmFeXFVUkQcAT54t8jscDbw67qLmIL1dcf6umN93eumxtWZuaK1cV4Ew2wWEePtLgebLayvO9bXHevr3kzU6KEkSVLBYJAkFQyGmTdadwEnYX3dsb7uWF/3el6j5xgkSQX3GCRJBYNBklQwGHogIs6PiJ9GxNMRsTcivtKmz2ci4mhE7K4eX+1zjfsjYk/12h/4Ktpo+MeImIiIJyPi4j7W9vGm9bI7It6IiOtb+vR1/UXEnRHxSkQ81dR2ZkQ8HBHPVn/PmGTaLVWfZyNiSx/r+4eI+GX17/fDiPjoJNNOuS3MYH1fj4hDTf+Gl08y7caI2Fdti1v7WN+9TbXtj4jdk0zbj/XX9j2lb9tgZvro8gGsBC6uhj8C/C+wrqXPZ4D/rLHG/cDZU4y/HPgxEMAngUdrqnMx8BKNG29qW3/Ap4GLgaea2r4JbK2GtwI3t5nuTOC56u8Z1fAZfarvc8CSavjmdvV1si3MYH1fB/66g3//XwG/A5wG/KL1/9JM1dcy/lvAV2tcf23fU/q1DbrH0AOZeTgzn6iGfwM8A5xbb1XTtgn4t2z4OfDRiFhZQx0bgF9lZq13smfmz4DXWpo3AXdXw3cDV7aZ9FLg4cx8LTNfBx4GNvajvsz8SWYer57+HDiv16/bqUnWXycuASYy87nMfAvYTmO999RU9UVEAF8Evtfr1+3UFO8pfdkGDYYei4gB4BPAo21G/15E/CIifhwRF/a1MEjgJxGxKyKG24w/F3ih6fmL1BNum5n8P2Sd6w/gnMw8XA2/BJzTps9sWY9forEH2M7JtoWZdF11qOvOSQ6DzIb19wfAy5n57CTj+7r+Wt5T+rINGgw9FBEfBr4PXJ+Zb7SMfoLG4ZHfBf4J+I8+l/epzLwYuAy4NiI+3efXP6mIOA34PPDvbUbXvf4K2dhnn5XXekfENuA4MDZJl7q2he8AHwMuAg7TOFwzG13N1HsLfVt/U72nzOQ2aDD0SEQspfEPOJaZP2gdn5lvZOb/VcMPAksj4ux+1ZeZh6q/rwA/pLHL3uwQcH7T8/Oqtn66DHgiM19uHVH3+qu8fOLwWvX3lTZ9al2PEfFnwB8BQ9Ubxwd0sC3MiMx8OTPfycx3ge9O8rp1r78lwB8D907Wp1/rb5L3lL5sgwZDD1THJO8AnsnMb0/S57eqfkTEJTTW/at9qu9DEfGRE8M0TlI+1dJtB/Cn1dVJnwSONu2y9sukn9TqXH9NdgAnrvDYAvyoTZ+HgM9FxBnVoZLPVW0zLiI2An8LfD4zj03Sp5NtYabqaz5n9YVJXvdxYG1ErKn2IDfTWO/98lngl5n5YruR/Vp/U7yn9GcbnMkz6wvlAXyKxi7dk8Du6nE58GXgy1Wf64C9NK6y+Dnw+32s73eq1/1FVcO2qr25vgBuo3FFyB5gsM/r8EM03uiXN7XVtv5oBNRh4G0ax2ivAc4CdgLPAv8NnFn1HQT+pWnaLwET1ePP+1jfBI1jyye2wX+u+v428OBU20Kf6run2raepPEGt7K1vur55TSuwvlVP+ur2u86sc019a1j/U32ntKXbdCvxJAkFTyUJEkqGAySpILBIEkqGAySpILBIEkqGAySpILBIEkq/D+TPNwHsxnEDgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "#test_loss_total = 0.0\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_cpu.forward(inputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "id": "JmLLZwL1Rfdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1c39fb-9acf-4be6-dbce-60085dff0b1d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0443\n",
            "\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "I5IM0lrXUdRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IqZeCHje12J_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Simple_MLP_HW2_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}